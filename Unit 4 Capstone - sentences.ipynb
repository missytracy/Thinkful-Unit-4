{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import state_union, stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval / Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2001-GWBush-1.txt',\n",
       " '2001-GWBush-2.txt',\n",
       " '2002-GWBush.txt',\n",
       " '2003-GWBush.txt',\n",
       " '2004-GWBush.txt',\n",
       " '2005-GWBush.txt',\n",
       " '2006-GWBush.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make list of individual file ids\n",
    "file_ids = state_union.fileids()\n",
    "file_ids[58:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create blank labels list to populate\n",
    "labels = []\n",
    "\n",
    "# Iterate through list of files pulling out President name & year\n",
    "for file in file_ids:\n",
    "    \n",
    "    # label president for each text.  Splice to extract name, remove non-alpha characters for \n",
    "    # occassions when a President gave more than one State of the Union address in the same year.\n",
    "    president = re.sub(\"[^a-zA-Z]\", '', file[5:-4])\n",
    "    \n",
    "    # Get year of each State of the Union Address\n",
    "    year = file[:4]\n",
    "    \n",
    "    # Append to labels list\n",
    "    labels.append([file, president, year])\n",
    "\n",
    "    \n",
    "# Create blank lists to populate with sentence level data\n",
    "sent_list = []\n",
    "pres_list = []\n",
    "date_list = []\n",
    "\n",
    "# Iterate through each State of the Union, create sentence level documents, maintain President\n",
    "# and year information for each sentence\n",
    "for i in range(len(labels)):\n",
    "    sents = state_union.sents(labels[i][0]) #get sentences from document\n",
    "    joined_sents = [(' '.join(sent), labels[i][1], labels[i][2])for sent in sents]\n",
    "    \n",
    "    # write out to individual lists for easier text processing\n",
    "    for i in range(len(joined_sents)): \n",
    "        sent_list.append(joined_sents[i][0]) \n",
    "        pres_list.append(joined_sents[i][1])  \n",
    "        date_list.append(joined_sents[i][2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'April          ',\n",
       " 'Mr   Speaker   Mr   President   Members of the Congress   It is with a heavy heart that I stand before you   my friends and colleagues   in the Congress of the United States  ',\n",
       " 'Only yesterday   we laid to rest the mortal remains of our beloved President   Franklin Delano Roosevelt  ']"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list to hold cleaned up sentences\n",
    "sent_list_clean = []\n",
    "\n",
    "# Iterate through sentence list, removing punctuation, numeric values,\n",
    "# frequently occuring word 'applause' that acts like a stop word in this \n",
    "# context, and sentences with all capital letters that represent title \n",
    "# information for each speech.\n",
    "for sent in sent_list:\n",
    "    sent = re.sub(\"[^a-zA-Z]\", ' ', sent) #remove numeric and punctuation\n",
    "    sent = re.sub(\"Applause\", '', sent)   #remove \"Applause\"\n",
    "    if sent == sent.upper():              #removes ALL CAPS header sentences\n",
    "        sent = \"\"                         #replace with blank\n",
    "    sent_list_clean.append(sent)\n",
    "\n",
    "# Inspect cleaned sentences\n",
    "print(len(sent_list_clean))\n",
    "sent_list_clean[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'April',\n",
       " 'Mr Speaker Mr President Members of the Congress It is with a heavy heart that I stand before you my friend and colleague in the Congress of the United States',\n",
       " 'Only yesterday we laid to rest the mortal remains of our beloved President Franklin Delano Roosevelt',\n",
       " 'At a time like this word are inadequate']"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize, get lemmas, and rejoin back to sentence level doc\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create list to hold lemmatized sentences\n",
    "lemma_sents = []\n",
    "\n",
    "for sent in sent_list_clean:\n",
    "    words = word_tokenize(sent)                                 #tokenize each word \n",
    "    word_lemma = [lemmatizer.lemmatize(word) for word in words] #lemmatize each word\n",
    "    sent_lemma = ' '.join(word_lemma)                           #rejoin back into sentences\n",
    "    lemma_sents.append(sent_lemma)\n",
    "\n",
    "# Inspect results\n",
    "lemma_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>president</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Truman</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April</td>\n",
       "      <td>Truman</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr Speaker Mr President Members of the Congres...</td>\n",
       "      <td>Truman</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Only yesterday we laid to rest the mortal rema...</td>\n",
       "      <td>Truman</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At a time like this word are inadequate</td>\n",
       "      <td>Truman</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent president  year\n",
       "0                                                       Truman  1945\n",
       "1                                              April    Truman  1945\n",
       "2  Mr Speaker Mr President Members of the Congres...    Truman  1945\n",
       "3  Only yesterday we laid to rest the mortal rema...    Truman  1945\n",
       "4            At a time like this word are inadequate    Truman  1945"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send to data frame\n",
    "df = pd.DataFrame()\n",
    "df['sent'] = lemma_sents\n",
    "df['president'] = pres_list\n",
    "df['year'] = date_list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         True\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "5        False\n",
       "6        False\n",
       "7        False\n",
       "8        False\n",
       "9        False\n",
       "10       False\n",
       "11       False\n",
       "12       False\n",
       "13       False\n",
       "14       False\n",
       "15       False\n",
       "16       False\n",
       "17       False\n",
       "18       False\n",
       "19       False\n",
       "20       False\n",
       "21       False\n",
       "22       False\n",
       "23       False\n",
       "24       False\n",
       "25       False\n",
       "26       False\n",
       "27       False\n",
       "28       False\n",
       "29       False\n",
       "         ...  \n",
       "17900    False\n",
       "17901    False\n",
       "17902    False\n",
       "17903    False\n",
       "17904    False\n",
       "17905    False\n",
       "17906     True\n",
       "17907    False\n",
       "17908    False\n",
       "17909    False\n",
       "17910     True\n",
       "17911    False\n",
       "17912     True\n",
       "17913    False\n",
       "17914    False\n",
       "17915    False\n",
       "17916    False\n",
       "17917    False\n",
       "17918    False\n",
       "17919    False\n",
       "17920    False\n",
       "17921    False\n",
       "17922    False\n",
       "17923    False\n",
       "17924    False\n",
       "17925    False\n",
       "17926    False\n",
       "17927    False\n",
       "17928    False\n",
       "17929     True\n",
       "Name: sent, Length: 17930, dtype: bool"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>president</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>April</td>\n",
       "      <td>Truman</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr Speaker Mr President Members of the Congres...</td>\n",
       "      <td>Truman</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Only yesterday we laid to rest the mortal rema...</td>\n",
       "      <td>Truman</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At a time like this word are inadequate</td>\n",
       "      <td>Truman</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The most eloquent tribute would be a reverent ...</td>\n",
       "      <td>Truman</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent president  year\n",
       "1                                              April    Truman  1945\n",
       "2  Mr Speaker Mr President Members of the Congres...    Truman  1945\n",
       "3  Only yesterday we laid to rest the mortal rema...    Truman  1945\n",
       "4            At a time like this word are inadequate    Truman  1945\n",
       "5  The most eloquent tribute would be a reverent ...    Truman  1945"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove blank sentence documents created from removing 'applause'\n",
    "# Isn't actually working and doesnt' seem more efficient.  Dropna() doesn't work w/ object\n",
    "df = df[df.sent!=\"\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13007, 3)\n",
      "(4336, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data frame, reserving 25% for validation\n",
    "\n",
    "df_train, df_test = train_test_split(df,\n",
    "                                    stratify=df['president'],\n",
    "                                    test_size=0.25,\n",
    "                                    random_state=42)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             lowercase=True,\n",
    "                             max_df=0.1, #take 80% of df...isn't changing output\n",
    "                             min_df=2, #use words that appear atleast twice\n",
    "                             use_idf=True,\n",
    "                             smooth_idf=True, \n",
    "                             #max_features=3000\n",
    "                             )\n",
    "\n",
    "# Specify data to vectorize\n",
    "X_train = df_train['sent']\n",
    "X_test = df_test['sent']\n",
    "\n",
    "# Vectorize\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train).tocsr()\n",
    "X_test_tfidf = vectorizer.transform(X_test).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13007, 5965)\n",
      "(4336, 5965)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis / SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Reduce matrix size \n",
    "svd = TruncatedSVD(100)\n",
    "lsa = make_pipeline(svd, Normalizer())  #isn't tfidf normalized already?\n",
    "#lsa = make_pipeline(svd)\n",
    "\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "variance_explained = svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print('Percent variance captured by components:', total_variance*100)\n",
    "\n",
    "sent_by_component = pd.DataFrame(X_train_lsa, index=X_train)\n",
    "\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(sent_by_component.loc[:, i].sort_values(ascending=False)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Memory error.  Better to do on full speeches?\n",
    "#similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.linear_model\n",
    "# add svm classifier\n",
    "# drop MLP?\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13007, 5965) (13007,)\n",
      "(4336, 5965) (4336,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_tfidf\n",
    "Y_train = df_train['president']\n",
    "\n",
    "X_test = X_test_tfidf\n",
    "Y_test = df_test['president']\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(max_depth=5)\n",
    "rfc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val scores: [0.20998081 0.22695853 0.21782559 0.21969988 0.21918336]\n",
      "Train accuracy: 0.2205735373260552\n",
      "Test accuracy: 0.21702029520295202\n"
     ]
    }
   ],
   "source": [
    "print('Cross val scores:', cross_val_score(rfc, X_train, Y_train, cv=5))\n",
    "print('Train accuracy:', rfc.score(X_train, Y_train))\n",
    "print('Test accuracy:', rfc.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val scores: [0.35759376 0.3604608  0.36728625 0.36123512 0.35158287]\n",
      "Train accuracy: 0.9766490667063286\n",
      "Test accuracy: 0.14231541378541154\n"
     ]
    }
   ],
   "source": [
    "print('Cross val scores:', cross_val_score(mlp, X_train, Y_train, cv=5))\n",
    "print('Train accuracy:', mlp.score(X_train, Y_train))\n",
    "print('Test accuracy:', mlp.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val scores: [0.38579655 0.37826421 0.38647714 0.38014621 0.39098613]\n",
      "Train accuracy: 0.5294841239332667\n",
      "Test accuracy: 0.3842250922509225\n"
     ]
    }
   ],
   "source": [
    "print('Cross val scores:', cross_val_score(nb, X_train, Y_train, cv=5))\n",
    "print('Train accuracy:', nb.score(X_train, Y_train))\n",
    "print('Test accuracy:', nb.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
